---
title: "Prompt Experiment"
author: "JD"
date: "2024-05-23"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This markdown file creates the prompt parts, permutes them, packages the entire prompt together with an abstract, and batches them off. It's the entire thing. It even includes the function to read in OpenAI's returned output!

```{r libraries, message=FALSE}
library(glue)
library(httr)
library(jsonlite)
library(tidyverse)
```

# Prompt Permuter

```{r prompt-parts}
# Opening sentences
open_a <- "You are a sharp, conscientious, and unflagging researcher who skilled at evaluating scientific grants across all areas of study"
open_b <- "You are an intelligent, meticulous, and tireless researcher with expertise in reviewing scientific grants across various fields"
open_c <- "You are a bright, punctilious, and indefatigable researcher who is an expert at reading scientific grants across all disciplines"

# Definitions/descriptions of inclusion categories
incl_uni <- "A broader impact is classified as `universal` if anyone could at least in theory benefit from it. Public goods such as improving primary school teaching practices or developing a better municipal water filter are examples of universal impacts."
incl_adv <- "A broader impact is classified as `advantaged` if the primary benefit will be experienced by advantaged groups and/or maintain status hierarchies. Scientists, as well as wealthy people and institutions count as advantaged."
incl_inc <- "Along the inclusion dimension, a broader impact is `inclusive` if its main beneficiaries are marginalized or underrepresented people. Common examples of inclusive broader impacts are programs that help women, people of color, and people with disabilities advance in STEM fields."

# Definitions/descriptions of **immediacy** categories
immed_int <- "A broader impact is `intrinsic` if the broader impact is inherent to and inseparable from the main principal purpose of the grant. For example, if a project is developing carbon capture and sequestration technology, the research and societal benefits of reducing greenhouse gases overlap and thus the broader impact is intrinsic to the project."
immed_dir <- "Along the immediacy dimension, a broader impact is classified as `direct` if its impact flows directly from the research but is not the specific goal of the research. Training graduate students is a quintessential direct broader impact. For most research grants, training a graduate student is not the purpose of the research. Rather, researchers train graduate students in order to complete a research project. The training is directly related to the research, but it is not the point or purpose of the research."
immed_ext <- "Some broader impacts are `extrinsic`. These broader impacts are separate from the main intellectual merit of the research project, and often the project is only tenuously related to it. For example, if a cell biologist studying proteins creates a presentation for a local high school about STEM careers, the broader impact is extrinsic. The outreach to high school students is a separate endeavor that takes place outside, or is extrinsic to, the research."
```

The following function generates three prompts, the first with opening A, second with opening B, third with opening C. Underneath that deterministic design, the orders or inclusion and immediacy are randomly permuted. One hacky feature here is using a `while` loop to generate rows one-by-one, making sure that no two prompts are identical -- this ensures each abstract receives labels from three distinct prompts. If I hadn't done this, there would be decent probability that any given project outcome report would have at least two identical prompt permutations beneath the unique opening.

I've also hard-coded generation of *three* prompts. This is simply because there are three opening sentences.

Output is a `tibble`, because why not.

```{r}
generate_three_prompts <- function() {
  
  prompts_tbl <- tibble(condition = character(), prompt = character())
  not_enough_rows <- TRUE
  
  while (not_enough_rows){
  
  incl_randomization <- sample(c('incl_uni', 'incl_adv', 'incl_inc'))
  incl_condition <- paste(str_remove(incl_randomization, 'incl_'), collapse = '_')
  
  immed_randomization <- sample(c('immed_int', 'immed_dir', 'immed_ext'))
  immed_condition <- paste(str_remove(immed_randomization, 'immed_'), collapse = '_')
  
  condition <- paste(incl_condition, immed_condition, sep = '-')
  
  if(condition %in% prompts_tbl$condition) next
  
  incl_prompt <-
    paste(sapply(incl_randomization, get), collapse = ' ')
  immed_prompt <-
    paste(sapply(immed_randomization, get), collapse = ' ')
  
  prompt <- paste0(
    '. You will be presented with an NSF grant project outcome report. NSF grant project outcome reports contain information on both the intellectual merit of a project (how the project will advance science) and its broader impact (how the project will benefit society). Your task is to find and describe the broader impact, then code it along two dimensions: inclusion and immediacy. Pay close attention to the meanings of words in backticks below. The first dimension, inclusion, refers to who will primarily receive the benefits of the broader impact. ',
    incl_prompt,
    ' The second dimension, immediacy, refers to the centrality of the broader impact to the main part of research project. ',
    immed_prompt,
    ' Please code the following NSF grant along the two dimensions of inclusion and immediacy. Take a deep breath, reason step by step, and respond in machine-readable json output. Use the following format, writing your responses where the triple backticked json values are (but you should use double quotes since that is correct json formatting): [{"broader_impact_description": ```Write a one or two sentence description of broader impact staying as close as possible to the text.```, "principal_beneficiaries": ```Who are the primary beneficiaries of the broader impact. Are they mentioned explicitly or are you making an inference?```, "reasoning": ```In a sentence, relate the broader impact to the coding rubric```, "inclusion_code": ```Choose from {universal, advantaged, inclusive}```, "immediacy_code": ```Choose from {intrinsic, direct, extrinsic}```}] Note: if a grant has more than one explicitly mentioned broader impact, reason about each one separately and give each its own json response, separating them with a comma. Do not use any formatting such as newlines, backticks, or escaped characters.'
  )
  
  prompts_tbl <- bind_rows(prompts_tbl, tibble(condition = condition, prompt = prompt))
  if (nrow(prompts_tbl) == 3) {
    prompts_tbl$prompt <- paste0(c(open_a, open_b, open_c), prompts_tbl$prompt)
    return(prompts_tbl)
  }
  }
}
```

# Experiment Data Frame

Now I create 1,200 prompts, three for each project outcome report.

```{r}
set.seed(123)
prompts <- 
  replicate(400, generate_three_prompts(), simplify = FALSE) %>% 
  purrr::list_rbind()
```

Now I read in the 400 project outcome reports (first line), then repeat each one three times, then bind them together with the prompts and create a unique id for each prompt-POR combination.

```{r}
pors <- readRDS('por_tbl.rds')

experiment_tbl <- 
  slice(pors, rep(1:n(), each = 3)) %>% 
  bind_cols(prompts) %>% 
  mutate(batch_id = paste(award_id, condition, sep = ';'),
         .before = 1)

experiment_tbl$batch_id <- paste0(experiment_tbl$batch_id, '-', rep(letters[1:3], 400))
```

# Batching and Dispatching

This code gets everything read to send to OpenAI. The actual part where I interact with the API should eventually include also programmatically downloading the batches. When I wrote this for the post I had few enough that I could manually download them all in about a minute and I was in a time crunch, so I just did that.

The function immediately below formats a request to be send to OpenAI's `chat-completions` API. It defaults to using a temperature of 0.2 and returning just one system response. As of writing, the default temperature if the client doesn't specify otherwise is 1. The default number of response choices is 1.

```{r create_request}
endpoint_url <- "https://api.openai.com/v1/chat/completions"

## YOUR API KEY HERE
## OR, DO IT PROGRAMMATICALLY IF YOUR PROFESSIONAL
## api_key <- 

create_request <- function(custom_id, system_message, user_message, 
                           temp = 0.2, n_choices = 1) {
  list(
    custom_id = custom_id,
    method = "POST",
    url = "/v1/chat/completions",
    body = list(
      model = "gpt-4o",
      temperature = temp,
      n = n_choices,
      messages = list(
        list(role = "system", content = system_message),
        list(role = "user", content = user_message)
      )
    )
  )
}
```

The function below takes in three arguments that any batching function would have to take: ids, system messages (instructions, context), and user messages (i.e., prompts, here, project outcome reports). Hopefully this is fixed by the time you're reading this, but at the time of doing this experiment, the Batch API allowed for uses to have very few "enqueued" messages. This is why the function creates subbatches. You specify the first and the last indices to give the beginning and end of the subbatch. 

```{r eval=FALSE}
send_subbatch <- function(batch_ids, system_messages, user_messages, 
                          start_idx, end_idx, temp = 0.2, n_choices = 1) {
  
  requests <- vector(mode = 'list', length = end_idx - start_idx + 1)
  
  j <- 1L
  for (i in start_idx:end_idx){
    requests[[j]] <- create_request(batch_ids[[i]], system_messages[[i]], 
                                    user_messages[[i]], temp = temp, 
                                    n_choices = n_choices)
    j <- j + 1L
  }
  
  jsonl_fn <- glue("requests_{start_idx}_{end_idx}.jsonl")
  fileConn <- file(jsonl_fn, open = "w")
  for (request in requests) {
    writeLines(jsonlite::toJSON(request, auto_unbox = TRUE), fileConn)
  }
  
  close(fileConn)
  
  files_response <- POST(
    'https://api.openai.com/v1/files',
    add_headers(Authorization = paste("Bearer", api_key)),
    body = list(
      purpose = "batch",
      file = upload_file(jsonl_fn)
    ),
    encode = "multipart"
  )
  # Define the JSON body
  body <- list(
    input_file_id = content(files_response)$id,
    endpoint = "/v1/chat/completions",
    completion_window = "24h"
  )
  
  # Make the POST request
  batch_response <- POST(
    "https://api.openai.com/v1/batches",
    add_headers(
      Authorization = paste("Bearer", api_key),
      `Content-Type` = "application/json"
    ),
    body = body,
    encode = "json"
  )
  
  batch_response
}

## And a function that takes in the batch id and gets its status so that
## you can move on to the next subbatch when it's done

batch_status_parsed <- function(batch_response_id) {
  response_status <- GET(
    paste0("https://api.openai.com/v1/batches/",
           batch_response_id), 
    add_headers(
      Authorization = paste("Bearer", api_key),
      `Content-Type` = "application/json"
    )
  )
  
  content(response_status, as = 'parsed', encoding = 'UTF-8')
}
```

Below are two `while` loops that send a subbatch, check on them, and move on to the next subbatch

On thing this code could do is keep a log of the number of successes you've had at a given number of samples. So, roughly, if you've had success with enough of them but need to drop for a particular batch, you can go back up as soon as the unusually large one is done. This is definitely not robust code. It worked well-enough for me, though.

```{r eval=FALSE}
# If you've already done some, you'll set this to something other than 1
starting_idx <- 1
# The acceptable increment (i.e., how many requests you're sending) depends
# on how many tokens you're sending. You'll have to play around with it
increment <- 46
# Don't touch this!
last_run <- FALSE
temp <- 0.2
n_choices <- 2
## How many samples are there?
n_samples <- 1200

while (TRUE) {
  
  if (starting_idx >= n_samples - increment){
    increment <- n_samples - starting_idx
    last_run <- TRUE
  }
  
  batch_response <-
    send_subbatch(
      batch_ids = experiment_tbl$batch_id,
      system_messages = experiment_tbl$prompt,
      user_messages = experiment_tbl$por,
      start_idx = starting_idx,
      end_idx = starting_idx + increment,
      temp = temp,
      n_choices = n_choices
    )
  
  if (last_run == TRUE) break
  
  Sys.sleep(30)
  
  status <- batch_status_parsed(content(batch_response)$id)$status
  
  while (status != 'completed') {
    Sys.sleep(30)
    
    status <- batch_status_parsed(content(batch_response)$id)$status
    
    if (status == 'failed') {
      print('Oops!')
      break
    }
  } 
  
  # there are other reasons a batch might fail,
  # but, since
  # A. i've already got status and
  # B. so far in my case it's always been "Enqueued token limit reached"
  # ((which means the batch was too large))
  if (status == 'failed') increment <- increment - 5; Sys.sleep(30)
  
  if (status == 'completed'){
    starting_idx <- starting_idx + increment + 1
    print('We had a success!')
  }
  
}
```

I did once get an `Error in while (status != "completed") { : argument is of length zero` because `NULL` had been returned by `batch_status_parsed`. Weird stuff was going on at that point even on their website.

# Reading in Processed Batches

This was hell.

The code below, which should probably eventually be moved to a different document, reads in the `jsonl` files returned by `OpenAI`'s `chat-completions` API and returns a dataframe.

The first step is to convert the `.jsonl` files with their unlovable malformatting into nice `.txt` files

```{r eval=FALSE}
setwd('temp_02_n_2_openai_responses')
jsonls_fn <- list.files(pattern = 'jsonl$')
  
unfuck_text <- function(fucked_text) {
  fucked_text %>%
    str_remove_all('\\\\n') %>%
    str_replace_all('\\\\"', '"') %>%
    str_remove_all('```') %>%
    str_remove_all('\\s{2,}') %>%
    str_remove_all('json[l]*')
}

for (jsonl_fn in jsonls_fn){
  t <- readLines(jsonl_fn)
  writeLines(
    unfuck_text(t),
    con = str_replace(jsonl_fn, 'jsonl$', 'txt'))
}
```

Now with the 'nice' `.txt` files we can relatively easily excise the `json` output returned by `chat-completions`.

```{r eval=FALSE}
#
# helper functions
#

json2df <- function(match_matrix_entry){
  data.frame(t(unlist(fromJSON(match_matrix_entry))))
}

todf <- function(choice_text) {
  n_impacts <- str_count(choice_text, 'broader_impact_description')
  # the control flow isn't actually necessary, 
  # but i suspect it makes things faster
  # should what's if and what's else be flipped, though?
  if (n_impacts == 1) {
    json2df(choice_text) %>% mutate(n_bis = n_impacts)
  } else {
    choice_text <- str_replace_all(choice_text, ',\\s*\\{\"broader_', '&&&{\"broader_')
    choice_text <- str_split_1(choice_text, '&&&')
    # this is an experiment trailing thing
    map(choice_text, json2df) %>% list_rbind() %>% mutate(n_bis = n_impacts)
  }
}

#
# the big function!
#

txtfn2df <- function(fn) {
  json_readlines <- readLines(fn)
  json_readlines <- str_replace_all(json_readlines, '\\[([A-Z]+)\\]', '(\\1)')
  for (i in seq_along(json_readlines)) {
    # you'll want to get the condition
    
    condition <- str_match(json_readlines[[i]], '\\"custom_id\\": \\"(\\d+;[-a-z_]+)')[1, 2]
    
    match_matrix <-
      str_match_all(json_readlines[[i]], '\"content\": \"\\[(.*?)\\]') %>% .[[1]]
    
    # verify that this is always 2
    # correct, this has been verified
    # for future you, it's 2 because you set the n parameter (number of choices) to 2 in the API call
    # two_coders_question_mark[[i]] <- length(matches) -- that's how you checked
    matches <- match_matrix[, 2]
    
    if (i > 1) {
      
      coded_tbl <-
        bind_rows(
          coded_tbl,
          map2(matches, c(1, 2), ~ todf(.x) %>% mutate(ra = .y)) %>%
            list_rbind() %>%
            mutate(condition = condition)
        )
        
    }
    else {
      coded_tbl <-
        map2(matches, c(1, 2), ~ todf(.x) %>% mutate(ra = .y)) %>% 
        list_rbind() %>% 
        mutate(condition = condition)
    }
  }
  
  coded_tbl
}
```

```{r eval=FALSE}
setwd('temp_02_n_2_openai_responses')
txts_fn <- list.files(pattern = 'txt$')
second_coding_tbl <- map(txts_fn, txtfn2df) %>% list_rbind()

second_coding_tbl <- 
  select(second_coding_tbl, condition, ra, n_bis, inclusion_code, immediacy_code, 
         bi_desc = broader_impact_description, principal_beneficiaries, reasoning) %>% 
  arrange(condition)

# write_csv(second_coding_tbl, 'llm_coded400.csv')
```

# Appendix

## Other Batching Code

I didn't end up using for anything in the post, but I had developed this code semi-independently on a different computer to send subbatches. Next time I work with the `chat-completions` API I'll combine the subbatching and dispatcing above with what's below (assuming both attempts have unique merits).

```{r eval=FALSE}
grant_sample14_16 <- readRDS('grant_sample14_16.rds')
grant_sample14_16_abs <- grant_sample14_16[['grant_sample14_16_abs']]
grant_sample14_16_fn <-  grant_sample14_16[['grant_sample14_16_fn']]
```


```{r eval=FALSE}
endpoint_url <- "https://api.openai.com/v1/chat/completions"

instructions <- "You will be given a fragment of an abstract of an NSF grant application. Your job is to classify the fragment as either containing the project's broader impact (a statement the project's societal benefits) or is completely about the project's intellectual merit. Broader impacts include inclusion, STEM education and workforce, public engagement, societal well-being, national security, partnerships, economic competitiveness, infrastructure. Respond with a two-item list object. The first item is a dummy variable, 1 to indicate the presence of broader impact, 0 otherwise. The second list item is a short summary sentence describing broader impact if it is present. If the dummy variable is 0, simply return [0, NA]."

create_request <- function(custom_id, abstract_fragment) {
  list(
    custom_id = custom_id,
    method = "POST",
    url = "/v1/chat/completions",
    body = list(
      model = "gpt-4o",
      messages = list(
        list(role = "system", content = instructions),
        list(role = "user", content = abstract_fragment)
      )
    )
  )
}

send_subbatch <- function(idx_1, idx_2) {
  requests <-
    map2(grant_sample14_16_fn[idx_1:idx_2],
         grant_sample14_16_abs[idx_1:idx_2],
         create_request)
  
  jsonl_fn <- glue::glue("requests_{idx_1}_{idx_2}.jsonl")
  fileConn <- file(jsonl_fn, open = "w")
  for (request in requests) {
    writeLines(toJSON(request, auto_unbox = TRUE), fileConn)
  }
  
  close(fileConn)
  
  files_response <- POST(
    'https://api.openai.com/v1/files',
    add_headers(Authorization = paste("Bearer", api_key)),
    body = list(
      purpose = "batch",
      file = upload_file(jsonl_fn)
    ),
    encode = "multipart"
  )
  # Define the JSON body
  body <- list(
    input_file_id = content(files_response)$id,
    endpoint = "/v1/chat/completions",
    completion_window = "24h"
  )
  
  # Make the POST request
  batch_response <- POST(
    "https://api.openai.com/v1/batches",
    add_headers(
      Authorization = paste("Bearer", api_key),
      `Content-Type` = "application/json"
    ),
    body = body,
    encode = "json"
  )
  
  batch_response
}

get_batch_status <- function(batch_response_id) {
  response_status <- GET(
    paste0("https://api.openai.com/v1/batches/",
           batch_response_id), 
    add_headers(
      Authorization = paste("Bearer", api_key),
      `Content-Type` = "application/json"
    )
  )
  
  cat(content(response_status, as = 'text', encoding = 'UTF-8'))
}

batch_status_string <- function(batch_response_id) {
  response_status <- GET(
    paste0("https://api.openai.com/v1/batches/",
           batch_response_id), 
    add_headers(
      Authorization = paste("Bearer", api_key),
      `Content-Type` = "application/json"
    )
  )
  
  content(response_status, as = 'parsed', encoding = 'UTF-8')$status
}

count_completed <- function(batch_response_id) {
  response_status <- GET(
    paste0("https://api.openai.com/v1/batches/",
           batch_response_id), 
    add_headers(
      Authorization = paste("Bearer", api_key),
      `Content-Type` = "application/json"
    )
  )
  
  content(response_status, as = 'parsed', encoding = 'UTF-8')$request_counts$completed
}
```

### The Dis-batcher

Another title was going to be the 'Dis-Bachelor'

```{r eval=FALSE}
# start_idx <- 601L
increment <- 90L

no_fails <- TRUE

while (no_fails){
  current_batch_response <- send_subbatch(start_idx, (start_idx + increment))
  Sys.sleep(10)
  bss <- batch_status_string(content(current_batch_response)$id)
  attempts <- 0L
  completed_old <- 0L
  
  while (bss == "in_progress") {
    # what would be better is to only increment waiting if there hasn't been progress
    # specifically, once there's progress, ping once per minute
    Sys.sleep(pmin(attempts * 30, 800))
    bss <- batch_status_string(content(current_batch_response)$id)
    completed <- count_completed(content(current_batch_response)$id)
    if (completed == completed_old) {
      attempts <- attempts + 1
    }
    completed_old <- completed
    
  }
  
  if (bss == 'completed') start_idx <- start_idx + increment + 1L
  # This should maybe be different
  # I hadn't counted on a "fail mode" being that I still overshoot the limit
  # In that case, what it should do is (at least temporarily) reduce the increment
  if (bss == 'failed') no_fails <- FALSE
}
```
## Other Code for Reading in Output

Honstly I should probably just delete what's below, but it's easier to paste it into the appendix. This code read in output that I didn't end up using for the post (I had made a mistake in the prompt permuter that led to prompts with double the description of one of the dimensions and no description of the other dimension).

### Reading in and minimally processing data AHHHH

```{r eval=FALSE}
first_jsonl_path <- 'data/experiment_batches/batch_0mcdj68xutq2IytYqjOKj0j5_output.jsonl'
first_jsonl_as_txt <- 'data/experiment_batches/batch_0mcdj68xutq2IytYqjOKj0j5_output.txt'

first_jsonl_cleaned <- 
  readLines(first_jsonl_path) %>% 
  # str_remove_all(fixed('\n')) %>%
  # str_remove_all('\\n') %>%
  # str_remove_all('\\\n') %>%
  str_remove_all('\\\\n') %>%
  # str_remove_all('\\\\\n') %>%
  str_remove_all('\\\\') %>% 
  str_remove_all('json') %>% 
  str_remove_all("`") %>% 
  str_replace_all(fixed('\"'), '"')

first_jsonl_cleaned[1]

fromJSON(first_jsonl_cleaned[1])
```

```{r eval=FALSE}
first_jsonl_as_txt <- 'data/experiment_batches/batch_0mcdj68xutq2IytYqjOKj0j5_output.txt'

first_jsonl_cleaned_txt <- 
  readLines(first_jsonl_as_txt) %>% 
  # str_remove_all(fixed('\n')) %>%
  # str_remove_all('\\n') %>%
  # str_remove_all('\\\n') %>%
  str_remove_all('\\\\n') %>%
  # str_remove_all('\\\\\n') %>%
  str_remove_all('\\\\') %>% 
  str_remove_all('json') %>% 
  str_remove_all("`") %>% 
  str_replace_all(fixed('\"'), '"') %>% 
  str_remove_all('[^\x20-\x7E]+')

first_jsonl_cleaned_txt[1]

fromJSON(first_jsonl_cleaned_txt[1])
```


```{r eval=FALSE}
sr <- fromJSON(readLines(''))
class(sr)

# Function to process a single JSONL file
process_jsonl <- function(file_path) {
  # Read the file and correct the JSON in each line
  corrected_lines <- readLines(file_path) %>%
    str_replace_all("\\n", "") %>%
    str_replace_all("(?<=content\": \")```json", "") %>%
    str_replace_all("```", "") 

  # Parse the corrected JSON lines
  json_list <- map(corrected_lines, fromJSON, simplifyVector = FALSE)

  # Extract the relevant data
  extracted_data <- map_dfr(json_list, function(entry) {
    content_list <- entry$response$body$choices
    tibble(
      content = map_chr(content_list, ~ .x$message$content),
      file_name = basename(file_path)  # Add file name for tracking
    )
  })
  
  #Parse the content field as JSON now
  extracted_data %>% mutate(content = map_chr(content, fromJSON)) %>%
                          unnest_wider(content)

}

first_jsonl_path <- 'data/experiment_batches/batch_0mcdj68xutq2IytYqjOKj0j5_output.jsonl'

# 1. Read and Correct JSON
raw_lines <- readLines(first_jsonl_path)
corrected_lines <- raw_lines %>%
  # str_replace_all("\\n", "") %>%
  str_replace_all("\n", "") %>%
  str_replace_all("(?<=content\": \")```json", "") %>%
  str_replace_all("```", "")

# 2. Parse Corrected JSON
json_list <- map(corrected_lines, fromJSON, simplifyVector = FALSE)

extracted_data <- map_dfr(json_list, function(entry) {
  content_list <- entry$response$body$choices
  tibble(
    content = map_chr(content_list, ~ .x$message$content),
    file_name = basename(first_jsonl_path)  
  )
})

parsed_content <- extracted_data %>% 
    mutate(parsed_content = map(content, fromJSON)) %>%
    select(-content) %>%  # You can drop the original `content` if you want
    unnest_wider(parsed_content) # This will unnest the JSON to the dataframe

process_jsonl <- function(file_path) {
  # Read and correct JSON (remove \n and extra quotes)
  corrected_lines <- readLines(file_path) %>%
    str_replace_all("\\n", "") %>%
    str_replace_all("(?<=content\": \")```json", "") %>%
    str_replace_all("```", "") %>%
    str_replace_all("\\\\", "") %>% # Remove extra backslashes 
    str_replace_all('"', '') # Remove extra quotation marks

  # Parse corrected JSON
  json_list <- map(corrected_lines, fromJSON, simplifyVector = FALSE)

  # Extract the data
  extracted_data <- map_dfr(json_list, function(entry) {
    content_list <- entry$response$body$choices
    tibble(
      content = map_chr(content_list, ~ .x$message$content),
      file_name = basename(file_path)  
    )
  })
  
  # Return the extracted data for now
  extracted_data 
}

process_jsonl(first_jsonl_path)
```


```{r eval=FALSE}
setwd('data/experiment_batches')
files <- list.files()

df <- fromJSON(test_json)
df

test_json <- 
  readLines(files[1], 1) #%>%
  # map(., jsonlite::fromJSON) %>%
  # unlist()

cat(str_remove_all(test_json, fixed("\\n")))


df <- stream_in(file(files[1]))

View(df)

cat(df$response$body$choices[[2]]$message$content)
```

```{r eval=FALSE}
setwd('data/experiment_batches')
files <- list.files()

# Read the JSONL file into a list
json_list <- stream_in(file(files[[1]]), simplifyVector = FALSE)

# Extract relevant content and handle multiple responses
extracted_data <- map_dfr(json_list, function(entry) {
  content_list <- entry$response$body$choices 
  
  tibble(
    content = map_chr(content_list, ~ fromJSON(.x$message$content)$broader_impact_description)
  )
})
```

### Trying to clean ChatGPT's totally farkakte output

You'll have to redo the working directory here

```{r eval=FALSE}
setwd('data/experiment_batches')
jsonls_fn <- list.files(pattern = 'jsonl$')
  
unfuck_text <- function(fucked_text) {
  fucked_text %>%
    str_remove_all('\\\\n') %>%
    str_replace_all('\\\\"', '"') %>%
    str_remove_all('```') %>%
    str_remove_all('\\s{2,}') %>%
    str_remove_all('json[l]*')
}

for (jsonl_fn in jsonls_fn){
  t <- readLines(jsonl_fn)
  writeLines(
    unfuck_text(t),
    con = str_replace(jsonl_fn, 'jsonl$', 'txt'))
}

get_imm_incl_codes <- function(text, which_one){
  if (which_one == 'immediacy') {
    temp_matrix <-
    str_match_all(text, 'immediacy_code["\']: ["\']([a-z]+)') %>%
    `[[`(., 1)
  } else {
    temp_matrix <-
    str_match_all(text, 'inclusion_code["\']: ["\']([a-z]+)') %>%
    `[[`(., 1)
  }
  temp_matrix[, 2]
}

txt_file_to_tbl <- function(dot_txt) {
  text_txt <- readLines(dot_txt)
  
  ids <- vector('character', length(text_txt))
  inclusions <- vector('list', length(text_txt))
  immediacies <- vector('list', length(text_txt))
  
  for (i in seq_along(text_txt)) {
    ids[[i]] <-
      str_match(text_txt[[i]], '\\"custom_id\\": \\"(\\d+;[-a-z_]+)')[1, 2]
    immediacies[[i]] <- get_imm_incl_codes(text_txt[[i]], 'immediacy')
    inclusions[[i]] <- get_imm_incl_codes(text_txt[[i]], 'inclusion')
  }
  
  # You'll throw and error if any of these don't work
  test_for_equality <-
    all(map2_lgl(immediacies, inclusions, ~ length(.x) == length(.y)))
  all(map_lgl(immediacies, ~ length(.x) > 0))
  all(map_lgl(inclusions, ~ length(.x) > 0))
  
  
  tibble(
    id = rep(ids, times = map_int(immediacies, length)),
    immediacy = unlist(immediacies),
    inclusion = unlist(inclusions)
  ) %>%
    tidyr::separate_wider_delim(id, ';', names = c('grant', 'condition'), cols_remove = FALSE)
}
```

```{r eval=FALSE}
setwd('data/experiment_batches')
txt_fns <- list.files(pattern = 'txt$')

coded_tbl <- 
  map(txt_fns, txt_file_to_tbl) %>% 
  list_rbind()

coded_tbl <- 
  coded_tbl %>% 
  separate_wider_delim(condition, delim = '-', names = c('opening', 'order_inc', 'order_imm')) %>% 
  relocate(everything(), id)
```
